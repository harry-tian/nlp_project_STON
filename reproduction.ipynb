{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "ac05eeae",
      "metadata": {
        "id": "ac05eeae"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "import csv\n",
        "import sys\n",
        "import time\n",
        "from collections import Counter\n",
        "\n",
        "from transformers import BertTokenizer, BertModel, BertForMaskedLM\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "968d6a75",
      "metadata": {
        "id": "968d6a75"
      },
      "source": [
        "# Explore data set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "7abb5105",
      "metadata": {
        "id": "7abb5105"
      },
      "outputs": [],
      "source": [
        "NUM_ENTITY = 'multiple_entity'\n",
        "TYPE = 'sra'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "92866f0f",
      "metadata": {
        "id": "92866f0f"
      },
      "outputs": [],
      "source": [
        "if NUM_ENTITY == 'multiple_entity' and TYPE == 'sra':\n",
        "   data_dir = './data/combined_data/multiple_entity_distractor/BertBase/complete_data_For_MultipleEntityObjectDistractorAccuracyBertBase.csv'\n",
        "\n",
        "# data_dir = './complete_data_For_MultipleEntityObjectDistractorAccuracyBertBase.csv'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "e40e6ae3",
      "metadata": {
        "id": "e40e6ae3"
      },
      "outputs": [],
      "source": [
        "def ordered_items_to_list(items):\n",
        "    return candidate.strip('[').strip(']').replace(\"'\",'').replace(' ','').split(',')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "9c810cda",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9c810cda",
        "outputId": "4aef5a31-7f3c-4c46-b66e-7dd73e514b76"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No. 1 Sentence: Daniel works as a florist . For his job , Daniel sells [MASK] . Target: flowers\n",
            "No. 2 Sentence: Daniel has a sister and now works as a florist . For his job , Daniel sells [MASK] . Target: flowers\n",
            "No. 3 Sentence: Daniel has a sister , played basketball , and now works as a florist . For his job , Daniel sells [MASK] . Target: flowers\n",
            "No. 4 Sentence: Daniel has a sister , played basketball , sang in a choir , and now works as a florist . For his job , Daniel sells [MASK] . Target: flowers\n",
            "No. 5 Sentence: Sebastian works as an optician . For his job , Sebastian sells [MASK] . Target: glasses\n",
            "No. 6 Sentence: Sebastian has a sister and now works as an optician . For his job , Sebastian sells [MASK] . Target: glasses\n",
            "No. 7 Sentence: Sebastian has a sister , played basketball , and now works as an optician . For his job , Sebastian sells [MASK] . Target: glasses\n",
            "No. 8 Sentence: Sebastian has a sister , played basketball , sang in a choir , and now works as an optician . For his job , Sebastian sells [MASK] . Target: glasses\n",
            "No. 9 Sentence: Daniel works as a butcher . For his job , Daniel sells [MASK] . Target: meat\n"
          ]
        }
      ],
      "source": [
        "f = open(data_dir)\n",
        "reader = csv.DictReader(f, delimiter='\\t')\n",
        "\n",
        "ct = 0\n",
        "targets = []\n",
        "sentences = []\n",
        "candidates = []\n",
        "num_attractors = []\n",
        "pre_pred = []\n",
        "\n",
        "for row in reader:\n",
        "    sentence = row['sentence']\n",
        "    target = row['target_occupation']\n",
        "    candidate = row['ordered_items']\n",
        "    n_attractors = row['count_attractors']\n",
        "    rel_rank = float(row['relative_rank'])\n",
        "    \n",
        "    targets.append(target)\n",
        "    sentences.append(sentence)\n",
        "    candidates.append(ordered_items_to_list(candidate))\n",
        "    num_attractors.append(n_attractors)\n",
        "    if rel_rank == 1:\n",
        "        pre_pred.append(1)\n",
        "    else:\n",
        "        pre_pred.append(0)\n",
        "    ct += 1\n",
        "    if ct < 10:\n",
        "        print(\"No. {} Sentence: {} Target: {}\".format(ct, sentence, target))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "cfd99af0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cfd99af0",
        "outputId": "30929297-3557-4fa3-e019-802b7c02aa10"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of instances: 12896\n",
            "Counter({'flowers': 4128, 'paintings': 4128, 'fish': 4128, 'glasses': 4128, 'meat': 4128, 'bread': 4128, 'santiago': 4128, 'paris': 4128, 'beijing': 4128, 'warsaw': 4128, 'jakarta': 4128, 'helsinki': 4128, 'India': 4128, 'Egypt': 4128, 'France': 4128, 'Italy': 4128, 'Peru': 4128, 'Russia': 4128, 'goal': 512, 'touchdown': 512, 'run': 512, 'century': 512})\n",
            "Counter({'flowers': 688, 'glasses': 688, 'meat': 688, 'bread': 688, 'fish': 688, 'paintings': 688, 'santiago': 688, 'beijing': 688, 'helsinki': 688, 'paris': 688, 'jakarta': 688, 'warsaw': 688, 'india': 688, 'france': 688, 'egypt': 688, 'peru': 688, 'italy': 688, 'russia': 688, 'touchdown': 128, 'run': 128, 'goal': 128, 'century': 128})\n",
            "Counter({'3': 8832, '2': 3072, '1': 816, '0': 176})\n"
          ]
        }
      ],
      "source": [
        "targets_counter = Counter(targets)\n",
        "attractor_counter = Counter(num_attractors)\n",
        "\n",
        "candidate_targets = Counter()\n",
        "for candidate in candidates:\n",
        "    candidate_targets.update(candidate)\n",
        "\n",
        "\n",
        "print(f\"Number of instances: {ct}\")\n",
        "print(candidate_targets)\n",
        "print(targets_counter)\n",
        "print(attractor_counter)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3a6dd963",
      "metadata": {
        "id": "3a6dd963"
      },
      "source": [
        "# BERT masked word prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6207177c",
      "metadata": {
        "id": "6207177c"
      },
      "outputs": [],
      "source": [
        "def prepare_text(text, model):\n",
        "    \"\"\"\n",
        "    Input:\n",
        "        text: typically an instance of a sentence in the data.\n",
        "        model: can be 'BERT'\n",
        "    Output:\n",
        "        res: a string consisting of orginal tokens and start-of-sentence and sentence separators.\n",
        "    \"\"\"\n",
        "    res = []\n",
        "    if model == 'BERT':\n",
        "        res.append(\"[CLS]\")\n",
        "        res += text.strip().split()        \n",
        "        if \"[mask]\" in res:\n",
        "            res[res.index(\"[mask]\")] = \"[MASK]\"\n",
        "        #period_index = [ind for ind, tok in enumerate(res) if tok == '.']\n",
        "        #for i, ind in enumerate(period_index):\n",
        "        #    res.insert(ind + 1 + i, \"[SEP]\")\n",
        "        res.append(\"[SEP]\")\n",
        "    return \" \".join(res)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8b4fc75f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "8b4fc75f",
        "outputId": "058b7c93-0e65-46dd-9c0a-6bfe2466cb54"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "john visited the tower of pisa , sebastian visited peru , daniel visited france , and joe visited egypt . the country john traveled to was [mask] .\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'[CLS] john visited the tower of pisa , sebastian visited peru , daniel visited france , and joe visited egypt . the country john traveled to was [MASK] . [SEP]'"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(sentences[10000])\n",
        "prepare_text(sentences[10000], \"BERT\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bbd06cbf",
      "metadata": {
        "id": "bbd06cbf"
      },
      "source": [
        "## 1. Bert-Base-Uncased"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f0ed6050",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f0ed6050",
        "outputId": "46a0f280-5b06-41c1-da12-9eee62353c9c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertForMaskedLM.from_pretrained('bert-base-uncased').to(\"cuda\")\n",
        "model.eval()\n",
        "\n",
        "def predict_masked(text, candidates, verbose=False):\n",
        "    \"\"\"\n",
        "    Input:\n",
        "        text: a prepared instance of a sentence in the data.\n",
        "        candidates: candidate words for which to calculate probabilities.\n",
        "        verbose: whether to print text along with predicted probabilities\n",
        "    Output:\n",
        "        prediction: one of the candidates with highest predicted probability.\n",
        "        probs: a tensor of predicted probailities of each candidate.\n",
        "    \"\"\"\n",
        "    \n",
        "    cand_probs = []\n",
        "    \n",
        "    if verbose:\n",
        "        print(text)\n",
        "    tokenized_text = tokenizer.tokenize(text)\n",
        "    if \"[MASK]\" in tokenized_text:\n",
        "        masked_index = tokenized_text.index(\"[MASK]\")\n",
        "    elif \"[mask]\" in tokenized_text:\n",
        "        masked_index = tokenized_text.index(\"[mask]\")\n",
        "    else:\n",
        "        print(\"No masks found.\")\n",
        "        return -1, torch.ones(len(candidates)) * (-99)\n",
        "\n",
        "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
        "    tokens_tensors = torch.tensor([indexed_tokens]).cuda()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        outputs = model(tokens_tensors)\n",
        "        predictions = outputs[0]\n",
        "        probs = F.softmax(predictions[0, masked_index], dim=-1)\n",
        "        \n",
        "    \n",
        "    for cand in candidates:\n",
        "        cand_id = [tokenizer.convert_tokens_to_ids(cand)]\n",
        "        token_weight = probs[cand_id].float().item()\n",
        "        if verbose:\n",
        "            print(f\"    {cand} | weights: {token_weight:.4f}\")\n",
        "        cand_probs.append(token_weight)\n",
        "        \n",
        "    cand_probs = torch.tensor(cand_probs)\n",
        "    prediction = candidates[cand_probs.argmax().item()]\n",
        "    \n",
        "    return prediction, cand_probs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4951622f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4951622f",
        "outputId": "257c5437-b3e4-47c0-b03e-35ef0fd37a67"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CLS] john visited the tower of pisa , sebastian visited peru , daniel visited france , and joe visited egypt . the country john traveled to was [MASK] . [SEP]\n",
            "    Italy | weights: 0.0000\n",
            "    France | weights: 0.0000\n",
            "    Egypt | weights: 0.0000\n",
            "    Peru | weights: 0.0000\n",
            "    Russia | weights: 0.0000\n",
            "    India | weights: 0.0000\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "('Italy',\n",
              " tensor([3.2347e-05, 3.2347e-05, 3.2347e-05, 3.2347e-05, 3.2347e-05, 3.2347e-05]))"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "predict_masked(prepare_text(sentences[10000], \"BERT\"), candidates[10000], True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f47b8f21",
      "metadata": {
        "id": "f47b8f21"
      },
      "outputs": [],
      "source": [
        "texts = [prepare_text(text, \"BERT\") for text in sentences]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8b60cce9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8b60cce9",
        "outputId": "8245896b-38d4-4a3b-e26c-80dd4b026b4c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "processed: 200/12896\n",
            "============================================================\n",
            "processed: 400/12896\n",
            "============================================================\n",
            "processed: 600/12896\n",
            "============================================================\n",
            "processed: 800/12896\n",
            "============================================================\n",
            "processed: 1000/12896\n",
            "============================================================\n",
            "processed: 1200/12896\n",
            "============================================================\n",
            "processed: 1400/12896\n",
            "============================================================\n",
            "processed: 1600/12896\n",
            "============================================================\n",
            "processed: 1800/12896\n",
            "============================================================\n",
            "processed: 2000/12896\n",
            "============================================================\n",
            "processed: 2200/12896\n",
            "============================================================\n",
            "processed: 2400/12896\n",
            "============================================================\n",
            "processed: 2600/12896\n",
            "============================================================\n",
            "processed: 2800/12896\n",
            "============================================================\n",
            "processed: 3000/12896\n",
            "============================================================\n",
            "processed: 3200/12896\n",
            "============================================================\n",
            "processed: 3400/12896\n",
            "============================================================\n",
            "processed: 3600/12896\n",
            "============================================================\n",
            "processed: 3800/12896\n",
            "============================================================\n",
            "processed: 4000/12896\n",
            "============================================================\n",
            "processed: 4200/12896\n",
            "============================================================\n",
            "processed: 4400/12896\n",
            "============================================================\n",
            "processed: 4600/12896\n",
            "============================================================\n",
            "processed: 4800/12896\n",
            "============================================================\n",
            "processed: 5000/12896\n",
            "============================================================\n",
            "processed: 5200/12896\n",
            "============================================================\n",
            "processed: 5400/12896\n",
            "============================================================\n",
            "processed: 5600/12896\n",
            "============================================================\n",
            "processed: 5800/12896\n",
            "============================================================\n",
            "processed: 6000/12896\n",
            "============================================================\n",
            "processed: 6200/12896\n",
            "============================================================\n",
            "processed: 6400/12896\n",
            "============================================================\n",
            "processed: 6600/12896\n",
            "============================================================\n",
            "processed: 6800/12896\n",
            "============================================================\n",
            "processed: 7000/12896\n",
            "============================================================\n",
            "processed: 7200/12896\n",
            "============================================================\n",
            "processed: 7400/12896\n",
            "============================================================\n",
            "processed: 7600/12896\n",
            "============================================================\n",
            "processed: 7800/12896\n",
            "============================================================\n",
            "processed: 8000/12896\n",
            "============================================================\n",
            "processed: 8200/12896\n",
            "============================================================\n",
            "processed: 8400/12896\n",
            "============================================================\n",
            "processed: 8600/12896\n",
            "============================================================\n",
            "processed: 8800/12896\n",
            "============================================================\n",
            "processed: 9000/12896\n",
            "============================================================\n",
            "processed: 9200/12896\n",
            "============================================================\n",
            "processed: 9400/12896\n",
            "============================================================\n",
            "processed: 9600/12896\n",
            "============================================================\n",
            "processed: 9800/12896\n",
            "============================================================\n",
            "processed: 10000/12896\n",
            "============================================================\n",
            "processed: 10200/12896\n",
            "============================================================\n",
            "processed: 10400/12896\n",
            "============================================================\n",
            "processed: 10600/12896\n",
            "============================================================\n",
            "processed: 10800/12896\n",
            "============================================================\n",
            "processed: 11000/12896\n",
            "============================================================\n",
            "processed: 11200/12896\n",
            "============================================================\n",
            "processed: 11400/12896\n",
            "============================================================\n",
            "processed: 11600/12896\n",
            "============================================================\n",
            "processed: 11800/12896\n",
            "============================================================\n",
            "processed: 12000/12896\n",
            "============================================================\n",
            "processed: 12200/12896\n",
            "============================================================\n",
            "processed: 12400/12896\n",
            "============================================================\n",
            "processed: 12600/12896\n",
            "============================================================\n",
            "processed: 12800/12896\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "ct = 0\n",
        "pred_correct = [0] * len(texts)\n",
        "\n",
        "for i, (text, cand, target, pp) in enumerate(zip(texts, candidates, targets, pre_pred)):\n",
        "    ct += 1\n",
        "    pred, _ = predict_masked(text, cand, False)\n",
        "    if pred.lower() == target.lower():\n",
        "        pred_correct[i] = 1\n",
        "    if pred_correct[i] != pp:\n",
        "        if pp == 1:\n",
        "            print(f\"No. {ct}. {text} | predicted: {pred} | pretrained: {target}\")\n",
        "        else:\n",
        "            print(f\"No. {ct}. {text} | predicted: {pred} | pretrained: {-1}\")\n",
        "        \n",
        "    if ct % 200 == 0:\n",
        "        print(\"processed: {}/{}\".format(ct, len(sentences)))\n",
        "        print(\"=\" * 60)\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "86f72384",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "86f72384",
        "outputId": "a0e76668-c702-4a9a-e484-5349cc644fc5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy for 0 attractor(s): 0.9091\n",
            "Accuracy for 1 attractor(s): 0.2451\n",
            "Accuracy for 2 attractor(s): 0.3516\n",
            "Accuracy for 3 attractor(s): 0.4250\n"
          ]
        }
      ],
      "source": [
        "accuracy_0attractor = []\n",
        "accuracy_1attractor = []\n",
        "accuracy_2attractor = []\n",
        "accuracy_3attractor = []\n",
        "\n",
        "for i in range(len(num_attractors)):\n",
        "    n = int(num_attractors[i])\n",
        "    if n == 0:\n",
        "        accuracy_0attractor += [pred_correct[i]]\n",
        "    elif n == 1:\n",
        "        accuracy_1attractor += [pred_correct[i]]\n",
        "    elif n == 2:\n",
        "        accuracy_2attractor += [pred_correct[i]]\n",
        "    elif n == 3:\n",
        "        accuracy_3attractor += [pred_correct[i]]\n",
        "    else:\n",
        "        print(\"Instance {}: more attractor than 3?\".format(i))\n",
        "        \n",
        "        \n",
        "print(f\"Accuracy for 0 attractor(s): {sum(accuracy_0attractor) / len(accuracy_0attractor):.4f}\")\n",
        "print(f\"Accuracy for 1 attractor(s): {sum(accuracy_1attractor) / len(accuracy_1attractor):.4f}\")\n",
        "print(f\"Accuracy for 2 attractor(s): {sum(accuracy_2attractor) / len(accuracy_2attractor):.4f}\")\n",
        "print(f\"Accuracy for 3 attractor(s): {sum(accuracy_3attractor) / len(accuracy_3attractor):.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6f45158c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6f45158c",
        "outputId": "cc132637-b1c2-41c4-b1bd-64bd4d49edd9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy for 0 attractor(s): 0.9091\n",
            "Accuracy for 1 attractor(s): 0.2451\n",
            "Accuracy for 2 attractor(s): 0.3516\n",
            "Accuracy for 3 attractor(s): 0.4250\n"
          ]
        }
      ],
      "source": [
        "accuracy_0attractor = []\n",
        "accuracy_1attractor = []\n",
        "accuracy_2attractor = []\n",
        "accuracy_3attractor = []\n",
        "\n",
        "for i in range(len(num_attractors)):\n",
        "    n = int(num_attractors[i])\n",
        "    if n == 0:\n",
        "        accuracy_0attractor += [pre_pred[i]]\n",
        "    elif n == 1:\n",
        "        accuracy_1attractor += [pre_pred[i]]\n",
        "    elif n == 2:\n",
        "        accuracy_2attractor += [pre_pred[i]]\n",
        "    elif n == 3:\n",
        "        accuracy_3attractor += [pre_pred[i]]\n",
        "    else:\n",
        "        print(\"Instance {}: more attractor than 3?\".format(i))\n",
        "        \n",
        "        \n",
        "print(f\"Accuracy for 0 attractor(s): {sum(accuracy_0attractor) / len(accuracy_0attractor):.4f}\")\n",
        "print(f\"Accuracy for 1 attractor(s): {sum(accuracy_1attractor) / len(accuracy_1attractor):.4f}\")\n",
        "print(f\"Accuracy for 2 attractor(s): {sum(accuracy_2attractor) / len(accuracy_2attractor):.4f}\")\n",
        "print(f\"Accuracy for 3 attractor(s): {sum(accuracy_3attractor) / len(accuracy_3attractor):.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "88aa267a",
      "metadata": {
        "id": "88aa267a"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "85525cea",
      "metadata": {
        "id": "85525cea"
      },
      "source": [
        "## 2. Bert-Large-Uncased"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "tEAOJ-BYp3XG",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tEAOJ-BYp3XG",
        "outputId": "639f6904-6499-4085-aa2c-6853d72b7909"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No. 1 Sentence: Daniel works as a florist . For his job , Daniel sells [MASK] . Target: flowers\n",
            "No. 2 Sentence: Daniel has a sister and now works as a florist . For his job , Daniel sells [MASK] . Target: flowers\n",
            "No. 3 Sentence: Daniel has a sister , played basketball , and now works as a florist . For his job , Daniel sells [MASK] . Target: flowers\n",
            "No. 4 Sentence: Daniel has a sister , played basketball , sang in a choir , and now works as a florist . For his job , Daniel sells [MASK] . Target: flowers\n",
            "No. 5 Sentence: Sebastian works as an optician . For his job , Sebastian sells [MASK] . Target: glasses\n",
            "No. 6 Sentence: Sebastian has a sister and now works as an optician . For his job , Sebastian sells [MASK] . Target: glasses\n",
            "No. 7 Sentence: Sebastian has a sister , played basketball , and now works as an optician . For his job , Sebastian sells [MASK] . Target: glasses\n",
            "No. 8 Sentence: Sebastian has a sister , played basketball , sang in a choir , and now works as an optician . For his job , Sebastian sells [MASK] . Target: glasses\n",
            "No. 9 Sentence: Daniel works as a butcher . For his job , Daniel sells [MASK] . Target: meat\n"
          ]
        }
      ],
      "source": [
        "data_dir = './complete_data_For_MultipleEntityObjectDistractorAccuracyBertLarge.csv'\n",
        "f = open(data_dir)\n",
        "reader = csv.DictReader(f, delimiter='\\t')\n",
        "\n",
        "ct = 0\n",
        "targets = []\n",
        "sentences = []\n",
        "candidates = []\n",
        "num_attractors = []\n",
        "pre_pred = []\n",
        "\n",
        "for row in reader:\n",
        "    sentence = row['sentence']\n",
        "    target = row['target_occupation']\n",
        "    candidate = row['ordered_items']\n",
        "    n_attractors = row['count_attractors']\n",
        "    rel_rank = float(row['relative_rank'])\n",
        "    \n",
        "    targets.append(target)\n",
        "    sentences.append(sentence)\n",
        "    candidates.append(ordered_items_to_list(candidate))\n",
        "    num_attractors.append(n_attractors)\n",
        "    if rel_rank == 1:\n",
        "        pre_pred.append(1)\n",
        "    else:\n",
        "        pre_pred.append(0)\n",
        "    ct += 1\n",
        "    if ct < 10:\n",
        "        print(\"No. {} Sentence: {} Target: {}\".format(ct, sentence, target))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "67d527e0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "67d527e0",
        "outputId": "91ba8c6c-c591-47f9-8239-687ad57f2932"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased')\n",
        "model = BertForMaskedLM.from_pretrained('bert-large-uncased').to(\"cuda\")\n",
        "model.eval()\n",
        "\n",
        "def predict_masked(text, candidates, verbose=False):\n",
        "    \"\"\"\n",
        "    Input:\n",
        "        text: a prepared instance of a sentence in the data.\n",
        "        candidates: candidate words for which to calculate probabilities.\n",
        "        verbose: whether to print text along with predicted probabilities\n",
        "    Output:\n",
        "        prediction: one of the candidates with highest predicted probability.\n",
        "        probs: a tensor of predicted probailities of each candidate.\n",
        "    \"\"\"\n",
        "    \n",
        "    cand_probs = []\n",
        "    \n",
        "    if verbose:\n",
        "        print(text)\n",
        "    tokenized_text = tokenizer.tokenize(text)\n",
        "    if \"[MASK]\" in tokenized_text:\n",
        "        masked_index = tokenized_text.index(\"[MASK]\")\n",
        "    elif \"[mask]\" in tokenized_text:\n",
        "        masked_index = tokenized_text.index(\"[mask]\")\n",
        "    else:\n",
        "        print(\"No masks found.\")\n",
        "        return -1, torch.ones(len(candidates)) * (-99)\n",
        "\n",
        "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
        "    tokens_tensors = torch.tensor([indexed_tokens]).cuda()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        outputs = model(tokens_tensors)\n",
        "        predictions = outputs[0]\n",
        "    \n",
        "    probs = F.softmax(predictions[0, masked_index], dim=0)#-1)\n",
        "    \n",
        "    for cand in candidates:\n",
        "        cand_id = [tokenizer.convert_tokens_to_ids(cand)]\n",
        "        token_weight = probs[cand_id].float().item()\n",
        "        if verbose:\n",
        "            print(f\"    {cand} | weights: {token_weight:.4f}\")\n",
        "        cand_probs.append(token_weight)\n",
        "        \n",
        "    cand_probs = torch.tensor(cand_probs)\n",
        "    prediction = candidates[cand_probs.argmax().item()]\n",
        "    \n",
        "    return prediction, cand_probs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a07ceb42",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a07ceb42",
        "outputId": "f22caee2-d1bf-4f60-902c-58be0b67edfe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CLS] john visited the tower of pisa , sebastian visited peru , daniel visited france , and joe visited egypt . the country john traveled to was [MASK] . [SEP]\n",
            "    Italy | weights: 0.0000\n",
            "    India | weights: 0.0000\n",
            "    Egypt | weights: 0.0000\n",
            "    France | weights: 0.0000\n",
            "    Russia | weights: 0.0000\n",
            "    Peru | weights: 0.0000\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "('Italy',\n",
              " tensor([1.1538e-05, 1.1538e-05, 1.1538e-05, 1.1538e-05, 1.1538e-05, 1.1538e-05]))"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "predict_masked(prepare_text(sentences[10000], \"BERT\"), candidates[10000], True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6d574c23",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6d574c23",
        "outputId": "8a589142-cb77-4f6a-8a3a-59c1eeccf699"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "processed: 200/12896\n",
            "============================================================\n",
            "processed: 400/12896\n",
            "============================================================\n",
            "processed: 600/12896\n",
            "============================================================\n",
            "processed: 800/12896\n",
            "============================================================\n",
            "processed: 1000/12896\n",
            "============================================================\n",
            "processed: 1200/12896\n",
            "============================================================\n",
            "processed: 1400/12896\n",
            "============================================================\n",
            "processed: 1600/12896\n",
            "============================================================\n",
            "processed: 1800/12896\n",
            "============================================================\n",
            "processed: 2000/12896\n",
            "============================================================\n",
            "processed: 2200/12896\n",
            "============================================================\n",
            "processed: 2400/12896\n",
            "============================================================\n",
            "processed: 2600/12896\n",
            "============================================================\n",
            "processed: 2800/12896\n",
            "============================================================\n",
            "processed: 3000/12896\n",
            "============================================================\n",
            "processed: 3200/12896\n",
            "============================================================\n",
            "processed: 3400/12896\n",
            "============================================================\n",
            "processed: 3600/12896\n",
            "============================================================\n",
            "processed: 3800/12896\n",
            "============================================================\n",
            "processed: 4000/12896\n",
            "============================================================\n",
            "processed: 4200/12896\n",
            "============================================================\n",
            "processed: 4400/12896\n",
            "============================================================\n",
            "processed: 4600/12896\n",
            "============================================================\n",
            "processed: 4800/12896\n",
            "============================================================\n",
            "processed: 5000/12896\n",
            "============================================================\n",
            "processed: 5200/12896\n",
            "============================================================\n",
            "processed: 5400/12896\n",
            "============================================================\n",
            "processed: 5600/12896\n",
            "============================================================\n",
            "processed: 5800/12896\n",
            "============================================================\n",
            "processed: 6000/12896\n",
            "============================================================\n",
            "processed: 6200/12896\n",
            "============================================================\n",
            "processed: 6400/12896\n",
            "============================================================\n",
            "processed: 6600/12896\n",
            "============================================================\n",
            "processed: 6800/12896\n",
            "============================================================\n",
            "processed: 7000/12896\n",
            "============================================================\n",
            "processed: 7200/12896\n",
            "============================================================\n",
            "processed: 7400/12896\n",
            "============================================================\n",
            "processed: 7600/12896\n",
            "============================================================\n",
            "processed: 7800/12896\n",
            "============================================================\n",
            "processed: 8000/12896\n",
            "============================================================\n",
            "processed: 8200/12896\n",
            "============================================================\n",
            "processed: 8400/12896\n",
            "============================================================\n",
            "processed: 8600/12896\n",
            "============================================================\n",
            "processed: 8800/12896\n",
            "============================================================\n",
            "processed: 9000/12896\n",
            "============================================================\n",
            "processed: 9200/12896\n",
            "============================================================\n",
            "processed: 9400/12896\n",
            "============================================================\n",
            "processed: 9600/12896\n",
            "============================================================\n",
            "processed: 9800/12896\n",
            "============================================================\n",
            "processed: 10000/12896\n",
            "============================================================\n",
            "processed: 10200/12896\n",
            "============================================================\n",
            "processed: 10400/12896\n",
            "============================================================\n",
            "processed: 10600/12896\n",
            "============================================================\n",
            "processed: 10800/12896\n",
            "============================================================\n",
            "processed: 11000/12896\n",
            "============================================================\n",
            "processed: 11200/12896\n",
            "============================================================\n",
            "processed: 11400/12896\n",
            "============================================================\n",
            "processed: 11600/12896\n",
            "============================================================\n",
            "processed: 11800/12896\n",
            "============================================================\n",
            "processed: 12000/12896\n",
            "============================================================\n",
            "processed: 12200/12896\n",
            "============================================================\n",
            "processed: 12400/12896\n",
            "============================================================\n",
            "processed: 12600/12896\n",
            "============================================================\n",
            "processed: 12800/12896\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "texts = [prepare_text(text, \"BERT\") for text in sentences]\n",
        "  \n",
        "ct = 0\n",
        "pred_correct = [0] * len(texts)\n",
        "\n",
        "for i, (text, cand, target, pp) in enumerate(zip(texts, candidates, targets, pre_pred)):\n",
        "    ct += 1\n",
        "    pred, _ = predict_masked(text, cand, False)\n",
        "    if pred.lower() == target.lower():\n",
        "        pred_correct[i] = 1\n",
        "    if pred_correct[i] != pp:\n",
        "        if pp == 1:\n",
        "            print(f\"No. {ct}. {text} | predicted: {pred} | pretrained: {target}\")\n",
        "        else:\n",
        "            print(f\"No. {ct}. {text} | predicted: {pred} | pretrained: {-1}\")\n",
        "        \n",
        "    if ct % 200 == 0:\n",
        "        print(\"processed: {}/{}\".format(ct, len(sentences)))\n",
        "        print(\"=\" * 60)\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9fb3a2c0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9fb3a2c0",
        "outputId": "d748746f-1eb5-411b-c751-bca4abc4c12c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy for 0 attractor(s): 0.9545\n",
            "Accuracy for 1 attractor(s): 0.2598\n",
            "Accuracy for 2 attractor(s): 0.3763\n",
            "Accuracy for 3 attractor(s): 0.4537\n"
          ]
        }
      ],
      "source": [
        "accuracy_0attractor = []\n",
        "accuracy_1attractor = []\n",
        "accuracy_2attractor = []\n",
        "accuracy_3attractor = []\n",
        "\n",
        "for i in range(len(num_attractors)):\n",
        "    n = int(num_attractors[i])\n",
        "    if n == 0:\n",
        "        accuracy_0attractor += [pred_correct[i]]\n",
        "    elif n == 1:\n",
        "        accuracy_1attractor += [pred_correct[i]]\n",
        "    elif n == 2:\n",
        "        accuracy_2attractor += [pred_correct[i]]\n",
        "    elif n == 3:\n",
        "        accuracy_3attractor += [pred_correct[i]]\n",
        "    else:\n",
        "        print(\"Instance {}: more attractor than 3?\".format(i))\n",
        "        \n",
        "        \n",
        "print(f\"Accuracy for 0 attractor(s): {sum(accuracy_0attractor) / len(accuracy_0attractor):.4f}\")\n",
        "print(f\"Accuracy for 1 attractor(s): {sum(accuracy_1attractor) / len(accuracy_1attractor):.4f}\")\n",
        "print(f\"Accuracy for 2 attractor(s): {sum(accuracy_2attractor) / len(accuracy_2attractor):.4f}\")\n",
        "print(f\"Accuracy for 3 attractor(s): {sum(accuracy_3attractor) / len(accuracy_3attractor):.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6c5cfeb0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6c5cfeb0",
        "outputId": "83c92771-0f8b-4c80-fb38-123c09498de7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy for 0 attractor(s): 0.9545\n",
            "Accuracy for 1 attractor(s): 0.2598\n",
            "Accuracy for 2 attractor(s): 0.3763\n",
            "Accuracy for 3 attractor(s): 0.4537\n"
          ]
        }
      ],
      "source": [
        "accuracy_0attractor = []\n",
        "accuracy_1attractor = []\n",
        "accuracy_2attractor = []\n",
        "accuracy_3attractor = []\n",
        "\n",
        "for i in range(len(num_attractors)):\n",
        "    n = int(num_attractors[i])\n",
        "    if n == 0:\n",
        "        accuracy_0attractor += [pre_pred[i]]\n",
        "    elif n == 1:\n",
        "        accuracy_1attractor += [pre_pred[i]]\n",
        "    elif n == 2:\n",
        "        accuracy_2attractor += [pre_pred[i]]\n",
        "    elif n == 3:\n",
        "        accuracy_3attractor += [pre_pred[i]]\n",
        "    else:\n",
        "        print(\"Instance {}: more attractor than 3?\".format(i))\n",
        "        \n",
        "        \n",
        "print(f\"Accuracy for 0 attractor(s): {sum(accuracy_0attractor) / len(accuracy_0attractor):.4f}\")\n",
        "print(f\"Accuracy for 1 attractor(s): {sum(accuracy_1attractor) / len(accuracy_1attractor):.4f}\")\n",
        "print(f\"Accuracy for 2 attractor(s): {sum(accuracy_2attractor) / len(accuracy_2attractor):.4f}\")\n",
        "print(f\"Accuracy for 3 attractor(s): {sum(accuracy_3attractor) / len(accuracy_3attractor):.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "03b09563",
      "metadata": {
        "id": "03b09563"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "reproduction.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
