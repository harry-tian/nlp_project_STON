{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p-zw5P9ZkZV9",
        "outputId": "02ef5eaf-259c-4e1e-e622-469d82518436"
      },
      "id": "p-zw5P9ZkZV9",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.16.2-py3-none-any.whl (3.5 MB)\n",
            "\u001b[?25l\r\u001b[K     |                                | 10 kB 32.0 MB/s eta 0:00:01\r\u001b[K     |▏                               | 20 kB 32.4 MB/s eta 0:00:01\r\u001b[K     |▎                               | 30 kB 20.3 MB/s eta 0:00:01\r\u001b[K     |▍                               | 40 kB 17.2 MB/s eta 0:00:01\r\u001b[K     |▌                               | 51 kB 12.3 MB/s eta 0:00:01\r\u001b[K     |▌                               | 61 kB 14.3 MB/s eta 0:00:01\r\u001b[K     |▋                               | 71 kB 15.1 MB/s eta 0:00:01\r\u001b[K     |▊                               | 81 kB 15.1 MB/s eta 0:00:01\r\u001b[K     |▉                               | 92 kB 16.6 MB/s eta 0:00:01\r\u001b[K     |█                               | 102 kB 13.9 MB/s eta 0:00:01\r\u001b[K     |█                               | 112 kB 13.9 MB/s eta 0:00:01\r\u001b[K     |█                               | 122 kB 13.9 MB/s eta 0:00:01\r\u001b[K     |█▏                              | 133 kB 13.9 MB/s eta 0:00:01\r\u001b[K     |█▎                              | 143 kB 13.9 MB/s eta 0:00:01\r\u001b[K     |█▍                              | 153 kB 13.9 MB/s eta 0:00:01\r\u001b[K     |█▌                              | 163 kB 13.9 MB/s eta 0:00:01\r\u001b[K     |█▋                              | 174 kB 13.9 MB/s eta 0:00:01\r\u001b[K     |█▋                              | 184 kB 13.9 MB/s eta 0:00:01\r\u001b[K     |█▊                              | 194 kB 13.9 MB/s eta 0:00:01\r\u001b[K     |█▉                              | 204 kB 13.9 MB/s eta 0:00:01\r\u001b[K     |██                              | 215 kB 13.9 MB/s eta 0:00:01\r\u001b[K     |██                              | 225 kB 13.9 MB/s eta 0:00:01\r\u001b[K     |██▏                             | 235 kB 13.9 MB/s eta 0:00:01\r\u001b[K     |██▏                             | 245 kB 13.9 MB/s eta 0:00:01\r\u001b[K     |██▎                             | 256 kB 13.9 MB/s eta 0:00:01\r\u001b[K     |██▍                             | 266 kB 13.9 MB/s eta 0:00:01\r\u001b[K     |██▌                             | 276 kB 13.9 MB/s eta 0:00:01\r\u001b[K     |██▋                             | 286 kB 13.9 MB/s eta 0:00:01\r\u001b[K     |██▊                             | 296 kB 13.9 MB/s eta 0:00:01\r\u001b[K     |██▊                             | 307 kB 13.9 MB/s eta 0:00:01\r\u001b[K     |██▉                             | 317 kB 13.9 MB/s eta 0:00:01\r\u001b[K     |███                             | 327 kB 13.9 MB/s eta 0:00:01\r\u001b[K     |███                             | 337 kB 13.9 MB/s eta 0:00:01\r\u001b[K     |███▏                            | 348 kB 13.9 MB/s eta 0:00:01\r\u001b[K     |███▎                            | 358 kB 13.9 MB/s eta 0:00:01\r\u001b[K     |███▎                            | 368 kB 13.9 MB/s eta 0:00:01\r\u001b[K     |███▍                            | 378 kB 13.9 MB/s eta 0:00:01\r\u001b[K     |███▌                            | 389 kB 13.9 MB/s eta 0:00:01\r\u001b[K     |███▋                            | 399 kB 13.9 MB/s eta 0:00:01\r\u001b[K     |███▊                            | 409 kB 13.9 MB/s eta 0:00:01\r\u001b[K     |███▉                            | 419 kB 13.9 MB/s eta 0:00:01\r\u001b[K     |███▉                            | 430 kB 13.9 MB/s eta 0:00:01\r\u001b[K     |████                            | 440 kB 13.9 MB/s eta 0:00:01\r\u001b[K     |████                            | 450 kB 13.9 MB/s eta 0:00:01\r\u001b[K     |████▏                           | 460 kB 13.9 MB/s eta 0:00:01\r\u001b[K     |████▎                           | 471 kB 13.9 MB/s eta 0:00:01\r\u001b[K     |████▍                           | 481 kB 13.9 MB/s eta 0:00:01\r\u001b[K     |████▍                           | 491 kB 13.9 MB/s eta 0:00:01\r\u001b[K     |████▌                           | 501 kB 13.9 MB/s eta 0:00:01\r\u001b[K     |████▋                           | 512 kB 13.9 MB/s eta 0:00:01\r\u001b[K     |████▊                           | 522 kB 13.9 MB/s eta 0:00:01\r\u001b[K     |████▉                           | 532 kB 13.9 MB/s eta 0:00:01\r\u001b[K     |█████                           | 542 kB 13.9 MB/s eta 0:00:01\r\u001b[K     |█████                           | 552 kB 13.9 MB/s eta 0:00:01\r\u001b[K     |█████                           | 563 kB 13.9 MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 573 kB 13.9 MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 583 kB 13.9 MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 593 kB 13.9 MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 604 kB 13.9 MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 614 kB 13.9 MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 624 kB 13.9 MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 634 kB 13.9 MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 645 kB 13.9 MB/s eta 0:00:01\r\u001b[K     |██████                          | 655 kB 13.9 MB/s eta 0:00:01\r\u001b[K     |██████                          | 665 kB 13.9 MB/s eta 0:00:01\r\u001b[K     |██████                          | 675 kB 13.9 MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 686 kB 13.9 MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 696 kB 13.9 MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 706 kB 13.9 MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 716 kB 13.9 MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 727 kB 13.9 MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 737 kB 13.9 MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 747 kB 13.9 MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 757 kB 13.9 MB/s eta 0:00:01\r\u001b[K     |███████                         | 768 kB 13.9 MB/s eta 0:00:01\r\u001b[K     |███████                         | 778 kB 13.9 MB/s eta 0:00:01\r\u001b[K     |███████                         | 788 kB 13.9 MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 798 kB 13.9 MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 808 kB 13.9 MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 819 kB 13.9 MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 829 kB 13.9 MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 839 kB 13.9 MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 849 kB 13.9 MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 860 kB 13.9 MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 870 kB 13.9 MB/s eta 0:00:01\r\u001b[K     |████████                        | 880 kB 13.9 MB/s eta 0:00:01\r\u001b[K     |████████                        | 890 kB 13.9 MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 901 kB 13.9 MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 911 kB 13.9 MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 921 kB 13.9 MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 931 kB 13.9 MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 942 kB 13.9 MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 952 kB 13.9 MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 962 kB 13.9 MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 972 kB 13.9 MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 983 kB 13.9 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 993 kB 13.9 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 1.0 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 1.0 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 1.0 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 1.0 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 1.0 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 1.1 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 1.1 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 1.1 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 1.1 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 1.1 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 1.1 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 1.1 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 1.1 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 1.1 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 1.1 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 1.2 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 1.2 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 1.2 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 1.2 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 1.2 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 1.2 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 1.2 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 1.2 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 1.2 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 1.2 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 1.3 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 1.3 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 1.3 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 1.3 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 1.3 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 1.3 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 1.3 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 1.3 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 1.3 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 1.4 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 1.4 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 1.4 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 1.4 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 1.4 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 1.4 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 1.4 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 1.4 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 1.4 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 1.4 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 1.5 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 1.5 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 1.5 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 1.5 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 1.5 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 1.5 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 1.5 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 1.5 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 1.5 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 1.5 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 1.6 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 1.6 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 1.6 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 1.6 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 1.6 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 1.6 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 1.6 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 1.6 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 1.6 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 1.6 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 1.7 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 1.7 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 1.7 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 1.7 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 1.7 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 1.7 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 1.7 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 1.7 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 1.7 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 1.8 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 1.8 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 1.8 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 1.8 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 1.8 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 1.8 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 1.8 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 1.8 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 1.8 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 1.8 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 1.9 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 1.9 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 1.9 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 1.9 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 1.9 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 1.9 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 1.9 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 1.9 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 1.9 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 1.9 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 2.0 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 2.0 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 2.0 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 2.0 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 2.0 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 2.0 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 2.0 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 2.0 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 2.0 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 2.0 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 2.1 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 2.1 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 2.1 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 2.1 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 2.1 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 2.1 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 2.1 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 2.1 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 2.1 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 2.2 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 2.2 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 2.2 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 2.2 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 2.2 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 2.2 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 2.2 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 2.2 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 2.2 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 2.2 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 2.3 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 2.3 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 2.3 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 2.3 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 2.3 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 2.3 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 2.3 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 2.3 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 2.3 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 2.3 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 2.4 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 2.4 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 2.4 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 2.4 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 2.4 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 2.4 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 2.4 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 2.4 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 2.4 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 2.4 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 2.5 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 2.5 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 2.5 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 2.5 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 2.5 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 2.5 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 2.5 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 2.5 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 2.5 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 2.5 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 2.6 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 2.6 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 2.6 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 2.6 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 2.6 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 2.6 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 2.6 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 2.6 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 2.6 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 2.7 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 2.7 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 2.7 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 2.7 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 2.7 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 2.7 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 2.7 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 2.7 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 2.7 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 2.7 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 2.8 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 2.8 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 2.8 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 2.8 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 2.8 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 2.8 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 2.8 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 2.8 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 2.8 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 2.8 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 2.9 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 2.9 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 2.9 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 2.9 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 2.9 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 2.9 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 2.9 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 2.9 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 2.9 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 2.9 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 3.0 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 3.0 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 3.0 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 3.0 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 3.0 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 3.0 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 3.0 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 3.0 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 3.0 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 3.1 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 3.1 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 3.1 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 3.1 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 3.1 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 3.1 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 3.1 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 3.1 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 3.1 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 3.1 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 3.2 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 3.2 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 3.2 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 3.2 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 3.2 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 3.2 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 3.2 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 3.2 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 3.2 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 3.2 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 3.3 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 3.3 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 3.3 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 3.3 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 3.3 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 3.3 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 3.3 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 3.3 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 3.3 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 3.3 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 3.4 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 3.4 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 3.4 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 3.4 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 3.4 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 3.4 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 3.4 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 3.4 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 3.4 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 3.5 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 3.5 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 3.5 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 3.5 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 3.5 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 3.5 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 3.5 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 3.5 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 3.5 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 3.5 MB 13.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 3.5 MB 13.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.5)\n",
            "Collecting tokenizers!=0.11.3,>=0.10.1\n",
            "  Downloading tokenizers-0.11.6-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.5 MB 56.0 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.4.0-py3-none-any.whl (67 kB)\n",
            "\u001b[K     |████████████████████████████████| 67 kB 7.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.47-py2.py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 61.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 68.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.4.0 pyyaml-6.0 sacremoses-0.0.47 tokenizers-0.11.6 transformers-4.16.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "ac05eeae",
      "metadata": {
        "id": "ac05eeae"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "import csv\n",
        "import sys\n",
        "import time\n",
        "from collections import Counter\n",
        "\n",
        "from transformers import BertTokenizer, BertModel, BertForMaskedLM\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "968d6a75",
      "metadata": {
        "id": "968d6a75"
      },
      "source": [
        "# Explore data set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7abb5105",
      "metadata": {
        "id": "7abb5105"
      },
      "outputs": [],
      "source": [
        "NUM_ENTITY = 'multiple_entity'\n",
        "TYPE = 'sra'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "92866f0f",
      "metadata": {
        "id": "92866f0f"
      },
      "outputs": [],
      "source": [
        "#if NUM_ENTITY == 'multiple_entity' and TYPE == 'sra':\n",
        "#    data_dir = './data/combined_data/multiple_entity_distractor/BertBase/complete_data_For_MultipleEntityObjectDistractorAccuracyBertBase.csv'\n",
        "\n",
        "data_dir = './complete_data_For_MultipleEntityObjectDistractorAccuracyBertBase.csv'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "e40e6ae3",
      "metadata": {
        "id": "e40e6ae3"
      },
      "outputs": [],
      "source": [
        "def ordered_items_to_list(items):\n",
        "    return candidate.strip('[').strip(']').replace(\"'\",'').replace(' ','').split(',')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "9c810cda",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9c810cda",
        "outputId": "4aef5a31-7f3c-4c46-b66e-7dd73e514b76"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No. 1 Sentence: Daniel works as a florist . For his job , Daniel sells [MASK] . Target: flowers\n",
            "No. 2 Sentence: Daniel has a sister and now works as a florist . For his job , Daniel sells [MASK] . Target: flowers\n",
            "No. 3 Sentence: Daniel has a sister , played basketball , and now works as a florist . For his job , Daniel sells [MASK] . Target: flowers\n",
            "No. 4 Sentence: Daniel has a sister , played basketball , sang in a choir , and now works as a florist . For his job , Daniel sells [MASK] . Target: flowers\n",
            "No. 5 Sentence: Sebastian works as an optician . For his job , Sebastian sells [MASK] . Target: glasses\n",
            "No. 6 Sentence: Sebastian has a sister and now works as an optician . For his job , Sebastian sells [MASK] . Target: glasses\n",
            "No. 7 Sentence: Sebastian has a sister , played basketball , and now works as an optician . For his job , Sebastian sells [MASK] . Target: glasses\n",
            "No. 8 Sentence: Sebastian has a sister , played basketball , sang in a choir , and now works as an optician . For his job , Sebastian sells [MASK] . Target: glasses\n",
            "No. 9 Sentence: Daniel works as a butcher . For his job , Daniel sells [MASK] . Target: meat\n"
          ]
        }
      ],
      "source": [
        "f = open(data_dir)\n",
        "reader = csv.DictReader(f, delimiter='\\t')\n",
        "\n",
        "ct = 0\n",
        "targets = []\n",
        "sentences = []\n",
        "candidates = []\n",
        "num_attractors = []\n",
        "pre_pred = []\n",
        "\n",
        "for row in reader:\n",
        "    sentence = row['sentence']\n",
        "    target = row['target_occupation']\n",
        "    candidate = row['ordered_items']\n",
        "    n_attractors = row['count_attractors']\n",
        "    rel_rank = float(row['relative_rank'])\n",
        "    \n",
        "    targets.append(target)\n",
        "    sentences.append(sentence)\n",
        "    candidates.append(ordered_items_to_list(candidate))\n",
        "    num_attractors.append(n_attractors)\n",
        "    if rel_rank == 1:\n",
        "        pre_pred.append(1)\n",
        "    else:\n",
        "        pre_pred.append(0)\n",
        "    ct += 1\n",
        "    if ct < 10:\n",
        "        print(\"No. {} Sentence: {} Target: {}\".format(ct, sentence, target))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "cfd99af0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cfd99af0",
        "outputId": "30929297-3557-4fa3-e019-802b7c02aa10"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of instances: 12896\n",
            "Counter({'flowers': 4128, 'paintings': 4128, 'fish': 4128, 'glasses': 4128, 'meat': 4128, 'bread': 4128, 'santiago': 4128, 'paris': 4128, 'beijing': 4128, 'warsaw': 4128, 'jakarta': 4128, 'helsinki': 4128, 'India': 4128, 'Egypt': 4128, 'France': 4128, 'Italy': 4128, 'Peru': 4128, 'Russia': 4128, 'goal': 512, 'touchdown': 512, 'run': 512, 'century': 512})\n",
            "Counter({'flowers': 688, 'glasses': 688, 'meat': 688, 'bread': 688, 'fish': 688, 'paintings': 688, 'santiago': 688, 'beijing': 688, 'helsinki': 688, 'paris': 688, 'jakarta': 688, 'warsaw': 688, 'india': 688, 'france': 688, 'egypt': 688, 'peru': 688, 'italy': 688, 'russia': 688, 'touchdown': 128, 'run': 128, 'goal': 128, 'century': 128})\n",
            "Counter({'3': 8832, '2': 3072, '1': 816, '0': 176})\n"
          ]
        }
      ],
      "source": [
        "targets_counter = Counter(targets)\n",
        "attractor_counter = Counter(num_attractors)\n",
        "\n",
        "candidate_targets = Counter()\n",
        "for candidate in candidates:\n",
        "    candidate_targets.update(candidate)\n",
        "\n",
        "\n",
        "print(f\"Number of instances: {ct}\")\n",
        "print(candidate_targets)\n",
        "print(targets_counter)\n",
        "print(attractor_counter)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3a6dd963",
      "metadata": {
        "id": "3a6dd963"
      },
      "source": [
        "# BERT masked word prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "6207177c",
      "metadata": {
        "id": "6207177c"
      },
      "outputs": [],
      "source": [
        "def prepare_text(text, model):\n",
        "    \"\"\"\n",
        "    Input:\n",
        "        text: typically an instance of a sentence in the data.\n",
        "        model: can be 'BERT'\n",
        "    Output:\n",
        "        res: a string consisting of orginal tokens and start-of-sentence and sentence separators.\n",
        "    \"\"\"\n",
        "    res = []\n",
        "    if model == 'BERT':\n",
        "        res.append(\"[CLS]\")\n",
        "        res += text.strip().split()        \n",
        "        if \"[mask]\" in res:\n",
        "            res[res.index(\"[mask]\")] = \"[MASK]\"\n",
        "        #period_index = [ind for ind, tok in enumerate(res) if tok == '.']\n",
        "        #for i, ind in enumerate(period_index):\n",
        "        #    res.insert(ind + 1 + i, \"[SEP]\")\n",
        "        res.append(\"[SEP]\")\n",
        "    return \" \".join(res)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "8b4fc75f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "8b4fc75f",
        "outputId": "058b7c93-0e65-46dd-9c0a-6bfe2466cb54"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "john visited the tower of pisa , sebastian visited peru , daniel visited france , and joe visited egypt . the country john traveled to was [mask] .\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'[CLS] john visited the tower of pisa , sebastian visited peru , daniel visited france , and joe visited egypt . the country john traveled to was [MASK] . [SEP]'"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "print(sentences[10000])\n",
        "prepare_text(sentences[10000], \"BERT\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bbd06cbf",
      "metadata": {
        "id": "bbd06cbf"
      },
      "source": [
        "## 1. Bert-Base-Uncased"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "f0ed6050",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f0ed6050",
        "outputId": "46a0f280-5b06-41c1-da12-9eee62353c9c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertForMaskedLM.from_pretrained('bert-base-uncased').to(\"cuda\")\n",
        "model.eval()\n",
        "\n",
        "def predict_masked(text, candidates, verbose=False):\n",
        "    \"\"\"\n",
        "    Input:\n",
        "        text: a prepared instance of a sentence in the data.\n",
        "        candidates: candidate words for which to calculate probabilities.\n",
        "        verbose: whether to print text along with predicted probabilities\n",
        "    Output:\n",
        "        prediction: one of the candidates with highest predicted probability.\n",
        "        probs: a tensor of predicted probailities of each candidate.\n",
        "    \"\"\"\n",
        "    \n",
        "    cand_probs = []\n",
        "    \n",
        "    if verbose:\n",
        "        print(text)\n",
        "    tokenized_text = tokenizer.tokenize(text)\n",
        "    if \"[MASK]\" in tokenized_text:\n",
        "        masked_index = tokenized_text.index(\"[MASK]\")\n",
        "    elif \"[mask]\" in tokenized_text:\n",
        "        masked_index = tokenized_text.index(\"[mask]\")\n",
        "    else:\n",
        "        print(\"No masks found.\")\n",
        "        return -1, torch.ones(len(candidates)) * (-99)\n",
        "\n",
        "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
        "    tokens_tensors = torch.tensor([indexed_tokens]).cuda()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        outputs = model(tokens_tensors)\n",
        "        predictions = outputs[0]\n",
        "        probs = F.softmax(predictions[0, masked_index], dim=-1)\n",
        "        \n",
        "    \n",
        "    for cand in candidates:\n",
        "        cand_id = [tokenizer.convert_tokens_to_ids(cand)]\n",
        "        token_weight = probs[cand_id].float().item()\n",
        "        if verbose:\n",
        "            print(f\"    {cand} | weights: {token_weight:.4f}\")\n",
        "        cand_probs.append(token_weight)\n",
        "        \n",
        "    cand_probs = torch.tensor(cand_probs)\n",
        "    prediction = candidates[cand_probs.argmax().item()]\n",
        "    \n",
        "    return prediction, cand_probs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "4951622f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4951622f",
        "outputId": "257c5437-b3e4-47c0-b03e-35ef0fd37a67"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CLS] john visited the tower of pisa , sebastian visited peru , daniel visited france , and joe visited egypt . the country john traveled to was [MASK] . [SEP]\n",
            "    Italy | weights: 0.0000\n",
            "    France | weights: 0.0000\n",
            "    Egypt | weights: 0.0000\n",
            "    Peru | weights: 0.0000\n",
            "    Russia | weights: 0.0000\n",
            "    India | weights: 0.0000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('Italy',\n",
              " tensor([3.2347e-05, 3.2347e-05, 3.2347e-05, 3.2347e-05, 3.2347e-05, 3.2347e-05]))"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "predict_masked(prepare_text(sentences[10000], \"BERT\"), candidates[10000], True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "f47b8f21",
      "metadata": {
        "id": "f47b8f21"
      },
      "outputs": [],
      "source": [
        "texts = [prepare_text(text, \"BERT\") for text in sentences]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "8b60cce9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8b60cce9",
        "outputId": "8245896b-38d4-4a3b-e26c-80dd4b026b4c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "processed: 200/12896\n",
            "============================================================\n",
            "processed: 400/12896\n",
            "============================================================\n",
            "processed: 600/12896\n",
            "============================================================\n",
            "processed: 800/12896\n",
            "============================================================\n",
            "processed: 1000/12896\n",
            "============================================================\n",
            "processed: 1200/12896\n",
            "============================================================\n",
            "processed: 1400/12896\n",
            "============================================================\n",
            "processed: 1600/12896\n",
            "============================================================\n",
            "processed: 1800/12896\n",
            "============================================================\n",
            "processed: 2000/12896\n",
            "============================================================\n",
            "processed: 2200/12896\n",
            "============================================================\n",
            "processed: 2400/12896\n",
            "============================================================\n",
            "processed: 2600/12896\n",
            "============================================================\n",
            "processed: 2800/12896\n",
            "============================================================\n",
            "processed: 3000/12896\n",
            "============================================================\n",
            "processed: 3200/12896\n",
            "============================================================\n",
            "processed: 3400/12896\n",
            "============================================================\n",
            "processed: 3600/12896\n",
            "============================================================\n",
            "processed: 3800/12896\n",
            "============================================================\n",
            "processed: 4000/12896\n",
            "============================================================\n",
            "processed: 4200/12896\n",
            "============================================================\n",
            "processed: 4400/12896\n",
            "============================================================\n",
            "processed: 4600/12896\n",
            "============================================================\n",
            "processed: 4800/12896\n",
            "============================================================\n",
            "processed: 5000/12896\n",
            "============================================================\n",
            "processed: 5200/12896\n",
            "============================================================\n",
            "processed: 5400/12896\n",
            "============================================================\n",
            "processed: 5600/12896\n",
            "============================================================\n",
            "processed: 5800/12896\n",
            "============================================================\n",
            "processed: 6000/12896\n",
            "============================================================\n",
            "processed: 6200/12896\n",
            "============================================================\n",
            "processed: 6400/12896\n",
            "============================================================\n",
            "processed: 6600/12896\n",
            "============================================================\n",
            "processed: 6800/12896\n",
            "============================================================\n",
            "processed: 7000/12896\n",
            "============================================================\n",
            "processed: 7200/12896\n",
            "============================================================\n",
            "processed: 7400/12896\n",
            "============================================================\n",
            "processed: 7600/12896\n",
            "============================================================\n",
            "processed: 7800/12896\n",
            "============================================================\n",
            "processed: 8000/12896\n",
            "============================================================\n",
            "processed: 8200/12896\n",
            "============================================================\n",
            "processed: 8400/12896\n",
            "============================================================\n",
            "processed: 8600/12896\n",
            "============================================================\n",
            "processed: 8800/12896\n",
            "============================================================\n",
            "processed: 9000/12896\n",
            "============================================================\n",
            "processed: 9200/12896\n",
            "============================================================\n",
            "processed: 9400/12896\n",
            "============================================================\n",
            "processed: 9600/12896\n",
            "============================================================\n",
            "processed: 9800/12896\n",
            "============================================================\n",
            "processed: 10000/12896\n",
            "============================================================\n",
            "processed: 10200/12896\n",
            "============================================================\n",
            "processed: 10400/12896\n",
            "============================================================\n",
            "processed: 10600/12896\n",
            "============================================================\n",
            "processed: 10800/12896\n",
            "============================================================\n",
            "processed: 11000/12896\n",
            "============================================================\n",
            "processed: 11200/12896\n",
            "============================================================\n",
            "processed: 11400/12896\n",
            "============================================================\n",
            "processed: 11600/12896\n",
            "============================================================\n",
            "processed: 11800/12896\n",
            "============================================================\n",
            "processed: 12000/12896\n",
            "============================================================\n",
            "processed: 12200/12896\n",
            "============================================================\n",
            "processed: 12400/12896\n",
            "============================================================\n",
            "processed: 12600/12896\n",
            "============================================================\n",
            "processed: 12800/12896\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "ct = 0\n",
        "pred_correct = [0] * len(texts)\n",
        "\n",
        "for i, (text, cand, target, pp) in enumerate(zip(texts, candidates, targets, pre_pred)):\n",
        "    ct += 1\n",
        "    pred, _ = predict_masked(text, cand, False)\n",
        "    if pred.lower() == target.lower():\n",
        "        pred_correct[i] = 1\n",
        "    if pred_correct[i] != pp:\n",
        "        if pp == 1:\n",
        "            print(f\"No. {ct}. {text} | predicted: {pred} | pretrained: {target}\")\n",
        "        else:\n",
        "            print(f\"No. {ct}. {text} | predicted: {pred} | pretrained: {-1}\")\n",
        "        \n",
        "    if ct % 200 == 0:\n",
        "        print(\"processed: {}/{}\".format(ct, len(sentences)))\n",
        "        print(\"=\" * 60)\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "86f72384",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "86f72384",
        "outputId": "a0e76668-c702-4a9a-e484-5349cc644fc5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy for 0 attractor(s): 0.9091\n",
            "Accuracy for 1 attractor(s): 0.2451\n",
            "Accuracy for 2 attractor(s): 0.3516\n",
            "Accuracy for 3 attractor(s): 0.4250\n"
          ]
        }
      ],
      "source": [
        "accuracy_0attractor = []\n",
        "accuracy_1attractor = []\n",
        "accuracy_2attractor = []\n",
        "accuracy_3attractor = []\n",
        "\n",
        "for i in range(len(num_attractors)):\n",
        "    n = int(num_attractors[i])\n",
        "    if n == 0:\n",
        "        accuracy_0attractor += [pred_correct[i]]\n",
        "    elif n == 1:\n",
        "        accuracy_1attractor += [pred_correct[i]]\n",
        "    elif n == 2:\n",
        "        accuracy_2attractor += [pred_correct[i]]\n",
        "    elif n == 3:\n",
        "        accuracy_3attractor += [pred_correct[i]]\n",
        "    else:\n",
        "        print(\"Instance {}: more attractor than 3?\".format(i))\n",
        "        \n",
        "        \n",
        "print(f\"Accuracy for 0 attractor(s): {sum(accuracy_0attractor) / len(accuracy_0attractor):.4f}\")\n",
        "print(f\"Accuracy for 1 attractor(s): {sum(accuracy_1attractor) / len(accuracy_1attractor):.4f}\")\n",
        "print(f\"Accuracy for 2 attractor(s): {sum(accuracy_2attractor) / len(accuracy_2attractor):.4f}\")\n",
        "print(f\"Accuracy for 3 attractor(s): {sum(accuracy_3attractor) / len(accuracy_3attractor):.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "6f45158c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6f45158c",
        "outputId": "cc132637-b1c2-41c4-b1bd-64bd4d49edd9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy for 0 attractor(s): 0.9091\n",
            "Accuracy for 1 attractor(s): 0.2451\n",
            "Accuracy for 2 attractor(s): 0.3516\n",
            "Accuracy for 3 attractor(s): 0.4250\n"
          ]
        }
      ],
      "source": [
        "accuracy_0attractor = []\n",
        "accuracy_1attractor = []\n",
        "accuracy_2attractor = []\n",
        "accuracy_3attractor = []\n",
        "\n",
        "for i in range(len(num_attractors)):\n",
        "    n = int(num_attractors[i])\n",
        "    if n == 0:\n",
        "        accuracy_0attractor += [pre_pred[i]]\n",
        "    elif n == 1:\n",
        "        accuracy_1attractor += [pre_pred[i]]\n",
        "    elif n == 2:\n",
        "        accuracy_2attractor += [pre_pred[i]]\n",
        "    elif n == 3:\n",
        "        accuracy_3attractor += [pre_pred[i]]\n",
        "    else:\n",
        "        print(\"Instance {}: more attractor than 3?\".format(i))\n",
        "        \n",
        "        \n",
        "print(f\"Accuracy for 0 attractor(s): {sum(accuracy_0attractor) / len(accuracy_0attractor):.4f}\")\n",
        "print(f\"Accuracy for 1 attractor(s): {sum(accuracy_1attractor) / len(accuracy_1attractor):.4f}\")\n",
        "print(f\"Accuracy for 2 attractor(s): {sum(accuracy_2attractor) / len(accuracy_2attractor):.4f}\")\n",
        "print(f\"Accuracy for 3 attractor(s): {sum(accuracy_3attractor) / len(accuracy_3attractor):.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "88aa267a",
      "metadata": {
        "id": "88aa267a"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "85525cea",
      "metadata": {
        "id": "85525cea"
      },
      "source": [
        "## 2. Bert-Large-Uncased"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = './complete_data_For_MultipleEntityObjectDistractorAccuracyBertLarge.csv'\n",
        "f = open(data_dir)\n",
        "reader = csv.DictReader(f, delimiter='\\t')\n",
        "\n",
        "ct = 0\n",
        "targets = []\n",
        "sentences = []\n",
        "candidates = []\n",
        "num_attractors = []\n",
        "pre_pred = []\n",
        "\n",
        "for row in reader:\n",
        "    sentence = row['sentence']\n",
        "    target = row['target_occupation']\n",
        "    candidate = row['ordered_items']\n",
        "    n_attractors = row['count_attractors']\n",
        "    rel_rank = float(row['relative_rank'])\n",
        "    \n",
        "    targets.append(target)\n",
        "    sentences.append(sentence)\n",
        "    candidates.append(ordered_items_to_list(candidate))\n",
        "    num_attractors.append(n_attractors)\n",
        "    if rel_rank == 1:\n",
        "        pre_pred.append(1)\n",
        "    else:\n",
        "        pre_pred.append(0)\n",
        "    ct += 1\n",
        "    if ct < 10:\n",
        "        print(\"No. {} Sentence: {} Target: {}\".format(ct, sentence, target))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tEAOJ-BYp3XG",
        "outputId": "639f6904-6499-4085-aa2c-6853d72b7909"
      },
      "id": "tEAOJ-BYp3XG",
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No. 1 Sentence: Daniel works as a florist . For his job , Daniel sells [MASK] . Target: flowers\n",
            "No. 2 Sentence: Daniel has a sister and now works as a florist . For his job , Daniel sells [MASK] . Target: flowers\n",
            "No. 3 Sentence: Daniel has a sister , played basketball , and now works as a florist . For his job , Daniel sells [MASK] . Target: flowers\n",
            "No. 4 Sentence: Daniel has a sister , played basketball , sang in a choir , and now works as a florist . For his job , Daniel sells [MASK] . Target: flowers\n",
            "No. 5 Sentence: Sebastian works as an optician . For his job , Sebastian sells [MASK] . Target: glasses\n",
            "No. 6 Sentence: Sebastian has a sister and now works as an optician . For his job , Sebastian sells [MASK] . Target: glasses\n",
            "No. 7 Sentence: Sebastian has a sister , played basketball , and now works as an optician . For his job , Sebastian sells [MASK] . Target: glasses\n",
            "No. 8 Sentence: Sebastian has a sister , played basketball , sang in a choir , and now works as an optician . For his job , Sebastian sells [MASK] . Target: glasses\n",
            "No. 9 Sentence: Daniel works as a butcher . For his job , Daniel sells [MASK] . Target: meat\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "67d527e0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "67d527e0",
        "outputId": "91ba8c6c-c591-47f9-8239-687ad57f2932"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased')\n",
        "model = BertForMaskedLM.from_pretrained('bert-large-uncased').to(\"cuda\")\n",
        "model.eval()\n",
        "\n",
        "def predict_masked(text, candidates, verbose=False):\n",
        "    \"\"\"\n",
        "    Input:\n",
        "        text: a prepared instance of a sentence in the data.\n",
        "        candidates: candidate words for which to calculate probabilities.\n",
        "        verbose: whether to print text along with predicted probabilities\n",
        "    Output:\n",
        "        prediction: one of the candidates with highest predicted probability.\n",
        "        probs: a tensor of predicted probailities of each candidate.\n",
        "    \"\"\"\n",
        "    \n",
        "    cand_probs = []\n",
        "    \n",
        "    if verbose:\n",
        "        print(text)\n",
        "    tokenized_text = tokenizer.tokenize(text)\n",
        "    if \"[MASK]\" in tokenized_text:\n",
        "        masked_index = tokenized_text.index(\"[MASK]\")\n",
        "    elif \"[mask]\" in tokenized_text:\n",
        "        masked_index = tokenized_text.index(\"[mask]\")\n",
        "    else:\n",
        "        print(\"No masks found.\")\n",
        "        return -1, torch.ones(len(candidates)) * (-99)\n",
        "\n",
        "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
        "    tokens_tensors = torch.tensor([indexed_tokens]).cuda()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        outputs = model(tokens_tensors)\n",
        "        predictions = outputs[0]\n",
        "    \n",
        "    probs = F.softmax(predictions[0, masked_index], dim=0)#-1)\n",
        "    \n",
        "    for cand in candidates:\n",
        "        cand_id = [tokenizer.convert_tokens_to_ids(cand)]\n",
        "        token_weight = probs[cand_id].float().item()\n",
        "        if verbose:\n",
        "            print(f\"    {cand} | weights: {token_weight:.4f}\")\n",
        "        cand_probs.append(token_weight)\n",
        "        \n",
        "    cand_probs = torch.tensor(cand_probs)\n",
        "    prediction = candidates[cand_probs.argmax().item()]\n",
        "    \n",
        "    return prediction, cand_probs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "a07ceb42",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a07ceb42",
        "outputId": "f22caee2-d1bf-4f60-902c-58be0b67edfe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CLS] john visited the tower of pisa , sebastian visited peru , daniel visited france , and joe visited egypt . the country john traveled to was [MASK] . [SEP]\n",
            "    Italy | weights: 0.0000\n",
            "    India | weights: 0.0000\n",
            "    Egypt | weights: 0.0000\n",
            "    France | weights: 0.0000\n",
            "    Russia | weights: 0.0000\n",
            "    Peru | weights: 0.0000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('Italy',\n",
              " tensor([1.1538e-05, 1.1538e-05, 1.1538e-05, 1.1538e-05, 1.1538e-05, 1.1538e-05]))"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "predict_masked(prepare_text(sentences[10000], \"BERT\"), candidates[10000], True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "6d574c23",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6d574c23",
        "outputId": "8a589142-cb77-4f6a-8a3a-59c1eeccf699"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "processed: 200/12896\n",
            "============================================================\n",
            "processed: 400/12896\n",
            "============================================================\n",
            "processed: 600/12896\n",
            "============================================================\n",
            "processed: 800/12896\n",
            "============================================================\n",
            "processed: 1000/12896\n",
            "============================================================\n",
            "processed: 1200/12896\n",
            "============================================================\n",
            "processed: 1400/12896\n",
            "============================================================\n",
            "processed: 1600/12896\n",
            "============================================================\n",
            "processed: 1800/12896\n",
            "============================================================\n",
            "processed: 2000/12896\n",
            "============================================================\n",
            "processed: 2200/12896\n",
            "============================================================\n",
            "processed: 2400/12896\n",
            "============================================================\n",
            "processed: 2600/12896\n",
            "============================================================\n",
            "processed: 2800/12896\n",
            "============================================================\n",
            "processed: 3000/12896\n",
            "============================================================\n",
            "processed: 3200/12896\n",
            "============================================================\n",
            "processed: 3400/12896\n",
            "============================================================\n",
            "processed: 3600/12896\n",
            "============================================================\n",
            "processed: 3800/12896\n",
            "============================================================\n",
            "processed: 4000/12896\n",
            "============================================================\n",
            "processed: 4200/12896\n",
            "============================================================\n",
            "processed: 4400/12896\n",
            "============================================================\n",
            "processed: 4600/12896\n",
            "============================================================\n",
            "processed: 4800/12896\n",
            "============================================================\n",
            "processed: 5000/12896\n",
            "============================================================\n",
            "processed: 5200/12896\n",
            "============================================================\n",
            "processed: 5400/12896\n",
            "============================================================\n",
            "processed: 5600/12896\n",
            "============================================================\n",
            "processed: 5800/12896\n",
            "============================================================\n",
            "processed: 6000/12896\n",
            "============================================================\n",
            "processed: 6200/12896\n",
            "============================================================\n",
            "processed: 6400/12896\n",
            "============================================================\n",
            "processed: 6600/12896\n",
            "============================================================\n",
            "processed: 6800/12896\n",
            "============================================================\n",
            "processed: 7000/12896\n",
            "============================================================\n",
            "processed: 7200/12896\n",
            "============================================================\n",
            "processed: 7400/12896\n",
            "============================================================\n",
            "processed: 7600/12896\n",
            "============================================================\n",
            "processed: 7800/12896\n",
            "============================================================\n",
            "processed: 8000/12896\n",
            "============================================================\n",
            "processed: 8200/12896\n",
            "============================================================\n",
            "processed: 8400/12896\n",
            "============================================================\n",
            "processed: 8600/12896\n",
            "============================================================\n",
            "processed: 8800/12896\n",
            "============================================================\n",
            "processed: 9000/12896\n",
            "============================================================\n",
            "processed: 9200/12896\n",
            "============================================================\n",
            "processed: 9400/12896\n",
            "============================================================\n",
            "processed: 9600/12896\n",
            "============================================================\n",
            "processed: 9800/12896\n",
            "============================================================\n",
            "processed: 10000/12896\n",
            "============================================================\n",
            "processed: 10200/12896\n",
            "============================================================\n",
            "processed: 10400/12896\n",
            "============================================================\n",
            "processed: 10600/12896\n",
            "============================================================\n",
            "processed: 10800/12896\n",
            "============================================================\n",
            "processed: 11000/12896\n",
            "============================================================\n",
            "processed: 11200/12896\n",
            "============================================================\n",
            "processed: 11400/12896\n",
            "============================================================\n",
            "processed: 11600/12896\n",
            "============================================================\n",
            "processed: 11800/12896\n",
            "============================================================\n",
            "processed: 12000/12896\n",
            "============================================================\n",
            "processed: 12200/12896\n",
            "============================================================\n",
            "processed: 12400/12896\n",
            "============================================================\n",
            "processed: 12600/12896\n",
            "============================================================\n",
            "processed: 12800/12896\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "texts = [prepare_text(text, \"BERT\") for text in sentences]\n",
        "  \n",
        "ct = 0\n",
        "pred_correct = [0] * len(texts)\n",
        "\n",
        "for i, (text, cand, target, pp) in enumerate(zip(texts, candidates, targets, pre_pred)):\n",
        "    ct += 1\n",
        "    pred, _ = predict_masked(text, cand, False)\n",
        "    if pred.lower() == target.lower():\n",
        "        pred_correct[i] = 1\n",
        "    if pred_correct[i] != pp:\n",
        "        if pp == 1:\n",
        "            print(f\"No. {ct}. {text} | predicted: {pred} | pretrained: {target}\")\n",
        "        else:\n",
        "            print(f\"No. {ct}. {text} | predicted: {pred} | pretrained: {-1}\")\n",
        "        \n",
        "    if ct % 200 == 0:\n",
        "        print(\"processed: {}/{}\".format(ct, len(sentences)))\n",
        "        print(\"=\" * 60)\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "9fb3a2c0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9fb3a2c0",
        "outputId": "d748746f-1eb5-411b-c751-bca4abc4c12c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy for 0 attractor(s): 0.9545\n",
            "Accuracy for 1 attractor(s): 0.2598\n",
            "Accuracy for 2 attractor(s): 0.3763\n",
            "Accuracy for 3 attractor(s): 0.4537\n"
          ]
        }
      ],
      "source": [
        "accuracy_0attractor = []\n",
        "accuracy_1attractor = []\n",
        "accuracy_2attractor = []\n",
        "accuracy_3attractor = []\n",
        "\n",
        "for i in range(len(num_attractors)):\n",
        "    n = int(num_attractors[i])\n",
        "    if n == 0:\n",
        "        accuracy_0attractor += [pred_correct[i]]\n",
        "    elif n == 1:\n",
        "        accuracy_1attractor += [pred_correct[i]]\n",
        "    elif n == 2:\n",
        "        accuracy_2attractor += [pred_correct[i]]\n",
        "    elif n == 3:\n",
        "        accuracy_3attractor += [pred_correct[i]]\n",
        "    else:\n",
        "        print(\"Instance {}: more attractor than 3?\".format(i))\n",
        "        \n",
        "        \n",
        "print(f\"Accuracy for 0 attractor(s): {sum(accuracy_0attractor) / len(accuracy_0attractor):.4f}\")\n",
        "print(f\"Accuracy for 1 attractor(s): {sum(accuracy_1attractor) / len(accuracy_1attractor):.4f}\")\n",
        "print(f\"Accuracy for 2 attractor(s): {sum(accuracy_2attractor) / len(accuracy_2attractor):.4f}\")\n",
        "print(f\"Accuracy for 3 attractor(s): {sum(accuracy_3attractor) / len(accuracy_3attractor):.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "6c5cfeb0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6c5cfeb0",
        "outputId": "83c92771-0f8b-4c80-fb38-123c09498de7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy for 0 attractor(s): 0.9545\n",
            "Accuracy for 1 attractor(s): 0.2598\n",
            "Accuracy for 2 attractor(s): 0.3763\n",
            "Accuracy for 3 attractor(s): 0.4537\n"
          ]
        }
      ],
      "source": [
        "accuracy_0attractor = []\n",
        "accuracy_1attractor = []\n",
        "accuracy_2attractor = []\n",
        "accuracy_3attractor = []\n",
        "\n",
        "for i in range(len(num_attractors)):\n",
        "    n = int(num_attractors[i])\n",
        "    if n == 0:\n",
        "        accuracy_0attractor += [pre_pred[i]]\n",
        "    elif n == 1:\n",
        "        accuracy_1attractor += [pre_pred[i]]\n",
        "    elif n == 2:\n",
        "        accuracy_2attractor += [pre_pred[i]]\n",
        "    elif n == 3:\n",
        "        accuracy_3attractor += [pre_pred[i]]\n",
        "    else:\n",
        "        print(\"Instance {}: more attractor than 3?\".format(i))\n",
        "        \n",
        "        \n",
        "print(f\"Accuracy for 0 attractor(s): {sum(accuracy_0attractor) / len(accuracy_0attractor):.4f}\")\n",
        "print(f\"Accuracy for 1 attractor(s): {sum(accuracy_1attractor) / len(accuracy_1attractor):.4f}\")\n",
        "print(f\"Accuracy for 2 attractor(s): {sum(accuracy_2attractor) / len(accuracy_2attractor):.4f}\")\n",
        "print(f\"Accuracy for 3 attractor(s): {sum(accuracy_3attractor) / len(accuracy_3attractor):.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "03b09563",
      "metadata": {
        "id": "03b09563"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "name": "reproduction.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}