{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac05eeae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import csv\n",
    "import sys\n",
    "import time\n",
    "from collections import Counter\n",
    "\n",
    "from transformers import BertTokenizer, BertModel, BertForMaskedLM\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968d6a75",
   "metadata": {},
   "source": [
    "# Explore data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f397afce",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_ENTITY = 'multiple_entity'\n",
    "TYPE = 'sra'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5829a10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if NUM_ENTITY == 'multiple_entity' and TYPE == 'sra':\n",
    "    data_dir = './data/combined_data/multiple_entity_distractor/BertBase/complete_data_For_MultipleEntityObjectDistractorAccuracyBertBase.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51876059",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ordered_items_to_list(items):\n",
    "    return candidate.strip('[').strip(']').replace(\"'\",'').replace(' ','').split(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0fd7631",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. 1 Sentence: Daniel works as a florist . For his job , Daniel sells [MASK] . Target: flowers\n",
      "No. 2 Sentence: Daniel has a sister and now works as a florist . For his job , Daniel sells [MASK] . Target: flowers\n",
      "No. 3 Sentence: Daniel has a sister , played basketball , and now works as a florist . For his job , Daniel sells [MASK] . Target: flowers\n",
      "No. 4 Sentence: Daniel has a sister , played basketball , sang in a choir , and now works as a florist . For his job , Daniel sells [MASK] . Target: flowers\n",
      "No. 5 Sentence: Sebastian works as an optician . For his job , Sebastian sells [MASK] . Target: glasses\n",
      "No. 6 Sentence: Sebastian has a sister and now works as an optician . For his job , Sebastian sells [MASK] . Target: glasses\n",
      "No. 7 Sentence: Sebastian has a sister , played basketball , and now works as an optician . For his job , Sebastian sells [MASK] . Target: glasses\n",
      "No. 8 Sentence: Sebastian has a sister , played basketball , sang in a choir , and now works as an optician . For his job , Sebastian sells [MASK] . Target: glasses\n",
      "No. 9 Sentence: Daniel works as a butcher . For his job , Daniel sells [MASK] . Target: meat\n"
     ]
    }
   ],
   "source": [
    "f = open(data_dir)\n",
    "reader = csv.DictReader(f, delimiter='\\t')\n",
    "\n",
    "ct = 0\n",
    "targets = []\n",
    "sentences = []\n",
    "candidates = []\n",
    "num_attractors = []\n",
    "\n",
    "for row in reader:\n",
    "    sentence = row['sentence']\n",
    "    target = row['target_occupation']\n",
    "    candidate = row['ordered_items']\n",
    "    n_attractors = row['count_attractors']\n",
    "    \n",
    "    targets.append(target)\n",
    "    sentences.append(sentence)\n",
    "    candidates.append(ordered_items_to_list(candidate))\n",
    "    num_attractors.append(n_attractors)\n",
    "    ct += 1\n",
    "    if ct < 10:\n",
    "        print(\"No. {} Sentence: {} Target: {}\".format(ct, sentence, target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cfd99af0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of instances: 12896\n",
      "Counter({'flowers': 4128, 'paintings': 4128, 'fish': 4128, 'glasses': 4128, 'meat': 4128, 'bread': 4128, 'santiago': 4128, 'paris': 4128, 'beijing': 4128, 'warsaw': 4128, 'jakarta': 4128, 'helsinki': 4128, 'India': 4128, 'Egypt': 4128, 'France': 4128, 'Italy': 4128, 'Peru': 4128, 'Russia': 4128, 'goal': 512, 'touchdown': 512, 'run': 512, 'century': 512})\n",
      "Counter({'flowers': 688, 'glasses': 688, 'meat': 688, 'bread': 688, 'fish': 688, 'paintings': 688, 'santiago': 688, 'beijing': 688, 'helsinki': 688, 'paris': 688, 'jakarta': 688, 'warsaw': 688, 'india': 688, 'france': 688, 'egypt': 688, 'peru': 688, 'italy': 688, 'russia': 688, 'touchdown': 128, 'run': 128, 'goal': 128, 'century': 128})\n",
      "Counter({'3': 8832, '2': 3072, '1': 816, '0': 176})\n"
     ]
    }
   ],
   "source": [
    "targets_counter = Counter(targets)\n",
    "attractor_counter = Counter(num_attractors)\n",
    "\n",
    "candidate_targets = Counter()\n",
    "for candidate in candidates:\n",
    "    candidate_targets.update(candidate)\n",
    "\n",
    "\n",
    "print(f\"Number of instances: {ct}\")\n",
    "print(candidate_targets)\n",
    "print(targets_counter)\n",
    "print(attractor_counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6dd963",
   "metadata": {},
   "source": [
    "# BERT masked word prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be0f0570",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_text(text, model):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        text: typically an instance of a sentence in the data.\n",
    "        model: can be 'BERT'\n",
    "    Output:\n",
    "        res: a string consisting of orginal tokens and start-of-sentence and sentence separators.\n",
    "    \"\"\"\n",
    "    res = []\n",
    "    if model == 'BERT':\n",
    "        res.append(\"[CLS]\")\n",
    "        res += text.strip().split()        \n",
    "        if \"[mask]\" in res:\n",
    "            res[res.index(\"[mask]\")] = \"[MASK]\"\n",
    "        period_index = [ind for ind, tok in enumerate(res) if tok == '.']\n",
    "        for i, ind in enumerate(period_index):\n",
    "            res.insert(ind + 1 + i, \"[SEP]\")\n",
    "    return \" \".join(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8af5b7c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "john visited the tower of pisa , sebastian visited peru , daniel visited france , and joe visited egypt . the country john traveled to was [mask] .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'[CLS] john visited the tower of pisa , sebastian visited peru , daniel visited france , and joe visited egypt . [SEP] the country john traveled to was [MASK] . [SEP]'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(sentences[10000])\n",
    "prepare_text(sentences[10000], \"BERT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f0ed6050",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
    "model.eval()\n",
    "\n",
    "def predict_masked(text, candidates, verbose=False):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        text: a prepared instance of a sentence in the data.\n",
    "        candidates: candidate words for which to calculate probabilities.\n",
    "        verbose: whether to print text along with predicted probabilities\n",
    "    Output:\n",
    "        prediction: one of the candidates with highest predicted probability.\n",
    "        probs: a tensor of predicted probailities of each candidate.\n",
    "    \"\"\"\n",
    "    \n",
    "    cand_probs = []\n",
    "    \n",
    "    if verbose:\n",
    "        print(text)\n",
    "    tokenized_text = tokenizer.tokenize(text)\n",
    "    if \"[MASK]\" in tokenized_text:\n",
    "        masked_index = tokenized_text.index(\"[MASK]\")\n",
    "    elif \"[mask]\" in tokenized_text:\n",
    "        masked_index = tokenized_text.index(\"[mask]\")\n",
    "    else:\n",
    "        print(\"No masks found.\")\n",
    "        return -1, torch.ones(len(candidates)) * (-99)\n",
    "\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "    tokens_tensors = torch.tensor([indexed_tokens])\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(tokens_tensors)\n",
    "        predictions = outputs[0]\n",
    "    \n",
    "    probs = F.softmax(predictions[0, masked_index], dim=-1)\n",
    "    \n",
    "    for cand in candidates:\n",
    "        cand_id = [tokenizer.convert_tokens_to_ids(cand)]\n",
    "        token_weight = probs[cand_id].float().item()\n",
    "        if verbose:\n",
    "            print(f\"    {cand} | weights: {token_weight:.4f}\")\n",
    "        cand_probs.append(token_weight)\n",
    "        \n",
    "    cand_probs = torch.tensor(cand_probs)\n",
    "    prediction = candidates[cand_probs.argmax().item()]\n",
    "    \n",
    "    return prediction, cand_probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4951622f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] john visited the tower of pisa , sebastian visited peru , daniel visited france , and joe visited egypt . [SEP] the country john traveled to was [MASK] . [SEP]\n",
      "    Italy | weights: 0.0000\n",
      "    France | weights: 0.0000\n",
      "    Egypt | weights: 0.0000\n",
      "    Peru | weights: 0.0000\n",
      "    Russia | weights: 0.0000\n",
      "    India | weights: 0.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('Italy',\n",
       " tensor([2.9484e-05, 2.9484e-05, 2.9484e-05, 2.9484e-05, 2.9484e-05, 2.9484e-05]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_masked(prepare_text(sentences[10000], \"BERT\"), candidates[10000], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c138b104",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [prepare_text(text, \"BERT\") for text in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "18d01df2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed: 200/12896\n",
      "processed: 400/12896\n",
      "processed: 600/12896\n",
      "processed: 800/12896\n",
      "processed: 1000/12896\n",
      "processed: 1200/12896\n",
      "processed: 1400/12896\n",
      "processed: 1600/12896\n",
      "processed: 1800/12896\n",
      "processed: 2000/12896\n",
      "processed: 2200/12896\n",
      "processed: 2400/12896\n",
      "processed: 2600/12896\n",
      "processed: 2800/12896\n",
      "processed: 3000/12896\n",
      "processed: 3200/12896\n",
      "processed: 3400/12896\n",
      "processed: 3600/12896\n",
      "processed: 3800/12896\n",
      "processed: 4000/12896\n",
      "processed: 4200/12896\n",
      "processed: 4400/12896\n",
      "processed: 4600/12896\n",
      "processed: 4800/12896\n",
      "processed: 5000/12896\n",
      "processed: 5200/12896\n",
      "processed: 5400/12896\n",
      "processed: 5600/12896\n",
      "processed: 5800/12896\n",
      "processed: 6000/12896\n",
      "processed: 6200/12896\n",
      "processed: 6400/12896\n",
      "processed: 6600/12896\n",
      "processed: 6800/12896\n",
      "processed: 7000/12896\n",
      "processed: 7200/12896\n",
      "processed: 7400/12896\n",
      "processed: 7600/12896\n",
      "processed: 7800/12896\n",
      "processed: 8000/12896\n",
      "processed: 8200/12896\n",
      "processed: 8400/12896\n",
      "processed: 8600/12896\n",
      "processed: 8800/12896\n",
      "processed: 9000/12896\n",
      "processed: 9200/12896\n",
      "processed: 9400/12896\n",
      "processed: 9600/12896\n",
      "processed: 9800/12896\n",
      "processed: 10000/12896\n",
      "processed: 10200/12896\n",
      "processed: 10400/12896\n",
      "processed: 10600/12896\n",
      "processed: 10800/12896\n",
      "processed: 11000/12896\n",
      "processed: 11200/12896\n",
      "processed: 11400/12896\n",
      "processed: 11600/12896\n",
      "processed: 11800/12896\n",
      "processed: 12000/12896\n",
      "processed: 12200/12896\n",
      "processed: 12400/12896\n",
      "processed: 12600/12896\n",
      "processed: 12800/12896\n"
     ]
    }
   ],
   "source": [
    "ct = 0\n",
    "pred_correct = [0] * len(texts)\n",
    "\n",
    "for i, (text, cand, target) in enumerate(zip(texts, candidates, targets)):\n",
    "    ct += 1\n",
    "    pred, _ = predict_masked(text, cand, False)\n",
    "    if pred == target:\n",
    "        pred_correct[i] = 1\n",
    "    if ct % 200 == 0:\n",
    "        print(\"processed: {}/{}\".format(ct, len(sentences)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8f1bbb82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for 0 attractor(s): 0.6477\n",
      "Accuracy for 1 attractor(s): 0.0895\n",
      "Accuracy for 2 attractor(s): 0.0778\n",
      "Accuracy for 3 attractor(s): 0.0728\n"
     ]
    }
   ],
   "source": [
    "accuracy_0attractor = []\n",
    "accuracy_1attractor = []\n",
    "accuracy_2attractor = []\n",
    "accuracy_3attractor = []\n",
    "\n",
    "for i in range(len(num_attractors)):\n",
    "    n = int(num_attractors[i])\n",
    "    if n == 0:\n",
    "        accuracy_0attractor += [pred_correct[i]]\n",
    "    elif n == 1:\n",
    "        accuracy_1attractor += [pred_correct[i]]\n",
    "    elif n == 2:\n",
    "        accuracy_2attractor += [pred_correct[i]]\n",
    "    elif n == 3:\n",
    "        accuracy_3attractor += [pred_correct[i]]\n",
    "    else:\n",
    "        print(\"Instance {}: more attractor than 3?\".format(i))\n",
    "        \n",
    "        \n",
    "print(f\"Accuracy for 0 attractor(s): {sum(accuracy_0attractor) / len(accuracy_0attractor):.4f}\")\n",
    "print(f\"Accuracy for 1 attractor(s): {sum(accuracy_1attractor) / len(accuracy_1attractor):.4f}\")\n",
    "print(f\"Accuracy for 2 attractor(s): {sum(accuracy_2attractor) / len(accuracy_2attractor):.4f}\")\n",
    "print(f\"Accuracy for 3 attractor(s): {sum(accuracy_3attractor) / len(accuracy_3attractor):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993df7e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa43230",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6136a3a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d8d679",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs351",
   "language": "python",
   "name": "cs351"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
