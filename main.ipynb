{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71b7bbdf",
   "metadata": {},
   "source": [
    "# Performance of the LM models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84db16c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f237233",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(data_dir):\n",
    "    f = open(data_dir)\n",
    "    reader = csv.DictReader(f)\n",
    "\n",
    "    targets = []\n",
    "    sentences = []\n",
    "    for row in reader:\n",
    "        sentence = row['sentence']\n",
    "        target = row['answer']\n",
    "        \n",
    "        targets.append(target)\n",
    "        sentences.append(sentence)\n",
    "\n",
    "    return sentences, targets\n",
    "\n",
    "\n",
    "def generate(model, text, tokenizer, n=10):\n",
    "    # text = 'his name is Henry , her name is Mary , my name is Twyla and your name is Geneva . his name is'\n",
    "    input_ids = torch.tensor([tokenizer.encode(text)]).to(device)\n",
    "    output = model.generate(\n",
    "        input_ids,\n",
    "        do_sample = True, \n",
    "        max_length = len(input_ids[0]) + 1, \n",
    "        top_k = 50, \n",
    "        top_p = 0.95, \n",
    "        num_return_sequences = n\n",
    "    )\n",
    "    answers = tokenizer.decode(output[:,-1], skip_special_tokens = True)\n",
    "\n",
    "    return answers\n",
    "\n",
    "def eval(model, texts, targets, verbose = False):\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    errors = []\n",
    "\n",
    "    if \"roberta\" in model:\n",
    "        texts = [text.replace(\"[MASK]\", \"<mask>\") for text in texts]\n",
    "    elif \"gpt\" in model:\n",
    "        texts = [text.replace(\" [MASK] .\", \"\") for text in texts]\n",
    "\n",
    "    if \"bert\" in model:\n",
    "        if torch.cuda.is_available():\n",
    "            x = 0\n",
    "        else:\n",
    "            x = -1\n",
    "        predict = pipeline('fill-mask', model = model, device = x)\n",
    "        for text, target in tqdm(zip(texts, targets)):\n",
    "            total += 1\n",
    "            pred = predict(text)[0]\n",
    "            pred = pred[\"token_str\"]\n",
    "            if pred.strip().lower() == target.strip().lower():\n",
    "                correct += 1\n",
    "            else:\n",
    "                errors.append(target)\n",
    "            if verbose: \n",
    "                print(text, pred, target)\n",
    "\n",
    "    elif \"gpt\" in model:\n",
    "        tokenizer = GPT2Tokenizer.from_pretrained(model)\n",
    "        model = GPT2LMHeadModel.from_pretrained(model, pad_token_id=tokenizer.eos_token_id).to(device)\n",
    "        for text, target in tqdm(zip(texts, targets)):\n",
    "            total += 1\n",
    "            preds = generate(model, text, tokenizer, n=30)\n",
    "            if target.strip().lower() in preds.lower():\n",
    "                correct += 1\n",
    "            else:\n",
    "                errors.append(target)\n",
    "            if verbose: \n",
    "                print(target, preds)\n",
    "\n",
    "    return correct/total, errors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2597728f",
   "metadata": {},
   "source": [
    "## Name to entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64df63aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    models = [\"bert-base-uncased\", \"roberta-base\"]\n",
    "              #\"bert-large-uncased\", \n",
    "              #\"roberta-large\", \n",
    "              #\"gpt2\", \n",
    "              #\"gpt2-medium\"] \n",
    "    fieldnames = [\"model\", 0, 1, 2, 3]\n",
    "    results = []\n",
    "\n",
    "    data_dir = './'\n",
    "    out_dir = data_dir + \"name_results.csv\"\n",
    "\n",
    "    for model in models:\n",
    "        print(f\"{model}:\\n\" + 100 * '-')\n",
    "        result = dict.fromkeys(fieldnames)\n",
    "        result[\"model\"] = model\n",
    "        for n in [0, 1, 2, 3]:\n",
    "            print(f\"n={n}:\\n\" + 100 * '-')\n",
    "            f = data_dir + f\"name_num_attractors_{n}.csv\"\n",
    "            texts,targets = get_data(f)\n",
    "            acc, err = eval(model, texts, targets, verbose = False)\n",
    "            result[n] = acc\n",
    "        results.append(result)\n",
    "        print(results)\n",
    "\n",
    "    with open(out_dir, 'w', encoding='UTF8', newline='') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames = fieldnames)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34b7b26b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert-base-uncased:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "n=0:\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "369it [00:24, 15.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n=1:\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "186it [00:13, 13.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n=2:\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "186it [00:15, 11.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n=3:\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "186it [00:17, 10.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'model': 'bert-base-uncased', 0: 1.0, 1: 0.967741935483871, 2: 0.5967741935483871, 3: 0.6290322580645161}]\n",
      "roberta-base:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "n=0:\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "369it [00:25, 14.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n=1:\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "186it [00:14, 12.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n=2:\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "186it [00:17, 10.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n=3:\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "186it [00:19,  9.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'model': 'bert-base-uncased', 0: 1.0, 1: 0.967741935483871, 2: 0.5967741935483871, 3: 0.6290322580645161}, {'model': 'roberta-base', 0: 1.0, 1: 1.0, 2: 0.7580645161290323, 3: 0.5913978494623656}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc35406",
   "metadata": {},
   "source": [
    "## Parent-child"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e1f7ea08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    models = [\"bert-base-uncased\", \"roberta-base\"]\n",
    "              #\"bert-large-uncased\", \n",
    "              #\"roberta-large\", \n",
    "              #\"gpt2\", \n",
    "              #\"gpt2-medium\"] \n",
    "    fieldnames = [\"model\", 0, 1, 2, 3]\n",
    "    results = []\n",
    "\n",
    "    data_dir = './'\n",
    "    out_dir = data_dir + \"parents_results.csv\"\n",
    "\n",
    "    for model in models:\n",
    "        print(f\"{model}:\\n\" + 100 * '-')\n",
    "        result = dict.fromkeys(fieldnames)\n",
    "        result[\"model\"] = model\n",
    "        for n in [0, 1, 2, 3]:\n",
    "            print(f\"n={n}:\\n\" + 100 * '-')\n",
    "            f = data_dir + f\"parents_num_attractors_{n}.csv\"\n",
    "            texts,targets = get_data(f)\n",
    "            acc, err = eval(model, texts, targets, verbose = False)\n",
    "            result[n] = acc\n",
    "        results.append(result)\n",
    "        print(results)\n",
    "\n",
    "    with open(out_dir, 'w', encoding='UTF8', newline='') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames = fieldnames)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0dbc5f8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert-base-uncased:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "n=0:\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "369it [00:27, 13.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n=1:\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "245it [00:22, 11.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n=2:\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "245it [00:24,  9.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n=3:\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "245it [00:28,  8.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'model': 'bert-base-uncased', 0: 0.997289972899729, 1: 0.5428571428571428, 2: 0.46122448979591835, 3: 0.5551020408163265}]\n",
      "roberta-base:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "n=0:\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "369it [00:34, 10.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n=1:\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "245it [00:23, 10.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n=2:\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "245it [00:27,  9.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n=3:\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "245it [00:30,  8.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'model': 'bert-base-uncased', 0: 0.997289972899729, 1: 0.5428571428571428, 2: 0.46122448979591835, 3: 0.5551020408163265}, {'model': 'roberta-base', 0: 0.994579945799458, 1: 0.39591836734693875, 2: 0.3183673469387755, 3: 0.2816326530612245}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fac3a3e",
   "metadata": {},
   "source": [
    "## marriage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1d622bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    models = [\"bert-base-uncased\", \"roberta-base\"]\n",
    "              #\"bert-large-uncased\", \n",
    "              #\"roberta-large\", \n",
    "              #\"gpt2\", \n",
    "              #\"gpt2-medium\"] \n",
    "    fieldnames = [\"model\", 0, 1, 2, 3]\n",
    "    results = []\n",
    "\n",
    "    data_dir = './'\n",
    "    out_dir = data_dir + \"marriage_results.csv\"\n",
    "\n",
    "    for model in models:\n",
    "        print(f\"{model}:\\n\" + 100 * '-')\n",
    "        result = dict.fromkeys(fieldnames)\n",
    "        result[\"model\"] = model\n",
    "        for n in [0, 1, 2, 3]:\n",
    "            print(f\"n={n}:\\n\" + 100 * '-')\n",
    "            f = data_dir + f\"marry_num_attractors_{n}.csv\"\n",
    "            texts,targets = get_data(f)\n",
    "            acc, err = eval(model, texts, targets, verbose = False)\n",
    "            result[n] = acc\n",
    "        results.append(result)\n",
    "        print(results)\n",
    "\n",
    "    with open(out_dir, 'w', encoding='UTF8', newline='') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames = fieldnames)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7fdda48a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert-base-uncased:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "n=0:\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "245it [00:16, 15.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n=1:\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "245it [00:21, 11.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n=2:\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "245it [00:24, 10.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n=3:\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "245it [00:26,  9.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'model': 'bert-base-uncased', 0: 0.9877551020408163, 1: 0.4816326530612245, 2: 0.5755102040816327, 3: 0.6530612244897959}]\n",
      "roberta-base:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "n=0:\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "245it [00:18, 13.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n=1:\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "245it [00:21, 11.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n=2:\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "245it [00:25,  9.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n=3:\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "245it [00:28,  8.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'model': 'bert-base-uncased', 0: 0.9877551020408163, 1: 0.4816326530612245, 2: 0.5755102040816327, 3: 0.6530612244897959}, {'model': 'roberta-base', 0: 0.963265306122449, 1: 0.2979591836734694, 2: 0.5510204081632653, 3: 0.6122448979591837}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24dace3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs351",
   "language": "python",
   "name": "cs351"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
